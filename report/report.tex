\documentclass[a4paper,11pt, twoside, openright]{article}
\usepackage[english]{babel}
%\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{float}
\usepackage{fixltx2e}
\usepackage{listings}
\usepackage{enumitem}
\usepackage{color}
\usepackage{textcomp}
\usepackage{latexsym}
\usepackage{lstautogobble}
\usepackage[colorinlistoftodos]{todonotes}
\usepackage[margin=3cm]{geometry}
\usepackage{hyperref}
\usepackage{fancyhdr}
\usepackage{tikz}
\usepackage{libertine}
\usepackage{csquotes}
\usepackage[backend=bibtex, style=numeric]{biblatex}
\addbibresource{biblio.bib}
\hypersetup{
	hidelinks, 
	colorlinks = true,
	linkcolor = black,
	citecolor = black,
	urlcolor = blue
}

\pagestyle{fancy}
\lhead{\nouppercase{\leftmark}}
\rhead{\nouppercase{\rightmark}}
\chead{}
\lfoot{}
\cfoot{\thepage}
\rfoot{}
\renewcommand{\headrulewidth}{0.4pt}

\renewcommand{\ttdefault}{cmtt}
\begin{document}
	\clearpage
	\begin{titlepage}
		\centering
		\vspace*{\fill}
		{\scshape\LARGE Universit√† degli Studi di Verona \par}
		\vspace{1.5cm}
		\line(1,0){240} \\
		{\huge\bfseries Authorship Attribution\par}
		\line(1,0){240} \\
		\vspace{0.5cm}
		{\scshape\Large Big Data project report\par}
		\vspace{2cm}
		{\Large\itshape Davide Bianchi VR424505\par
		\Large\itshape Matteo Danzi VR424987\par}
		\vspace{1cm}
		\vspace{5cm}
		\vspace*{\fill}
		% Bottom of the page
		{\large A.A. 2018/2019\par}
	\end{titlepage}
	\thispagestyle{empty}
	\newpage
	\tableofcontents
	\newpage
	
	\section{Introduction}
	The project aim was to design a tool which could establish the authorship of a manuscript by using specific criteria described later. 
	The used architecture is based on Hadoop, a distributed filesystem simulator, running in a Docker container. The requirements of the project imposed the use of MapReduce programming model as seen in Laboratory Lessons of the Big Data course.

	\section{Background and System Description}
    \subsection{Cloudera Docker and system setup}
		A container is a standard unit of software that packages up code and all its dependencies so the application runs quickly and reliably from one computing environment to another. A Docker container image is an executable package of software that includes everything needed to run an application: code, runtime, system tools, system libraries and settings.

		\bigskip

		\noindent
		Docker images become containers when they run on Docker Engine. They isolate software from its environment and ensure that it works uniformly despite differences for instance between development and staging.
		
		\bigskip

		\noindent
		The Cloudera\parencite{Cloudera} Docker image used in this project contains an Hadoop Distributed File System (HDFS) partition. Cloudera provides a scalable, flexible, integrated platform that makes it easy to manage rapidly increasing volumes and varieties of data in an enterprise. 

		\bigskip
		
		\noindent
		Cloudera provides an HDFS (\textit{Hadoop Distributed FileSystem})\parencite{Hadoop-Mapreduce}, used to store files across server collections. The Hadoop version used in this project is \texttt{Hadoop 2.6.0-cdh5.7.0}.
		
		The distributed filesystem consists on two basic services: \begin{itemize}
			\item \textit{NameNode}: is a single instance service, running on a master node, which is responsible for files and directory mantainance within the filesystem itself;
			
			\item \textit{DataNode}: running on multiple nodes (many instances are allowed), are supposed to retrieve the application data when they are told by the NameNode.
		\end{itemize}
	
		\bigskip
		
		\noindent
		For the installation of Docker, the Cloudera Docker image and the execution of jobs using jar file it have been used the instructions of the course. The project is written using Java version \texttt{11.0.4}. For the code compilation we used IntelliJ version \texttt{2019.2.4} compiling using compatibility mode with version \texttt{1.7}.

	\subsection{Map Reduce Framework}
		MapReduce is a programming paradigm which works on parallel platforms in order to process in the best possible way large amounts of data. The client has to implement a map function, which is supposed to take as input raw data and process them into intermediate \textit{key-value} pairs, and the reduce function, which aim is to regroup key-value pairs relying on the key and generate key-value output pairs. The mapreduce model is explained more in detail in the next section.
		
		\bigskip


		\noindent
		This Programming Model consists of two main actions:
		\begin{itemize}
			\item \textbf{Map}: application of a function to every object in a list. Each object (e.g.: document) is independent.
			\begin{itemize}
				\item Order is not important
				\item Maps can be done in parallel since every object has no relation to each other
				\item The function produces an intermediate result
			\end{itemize}
			
			Formally the \textit{map} function can be described with the following notation: \[ map:(key_1, value_1) \to list(key_2, value_2) \]
			\item \textbf{Reduce}: combining the intermediate results generated by Maps to produce a final result.
		\end{itemize}

	\section{Project Workflow}

	The first step of the project is to analyze an amount of manuscripts, extracting relevant information from them in order to create a ``dictionary'' of known authors. Starting from these data, the program should take unknown manuscripts as input and establish the authorship by the comparing them with the ``dictionary'' previously created. All the manuscripts are \texttt{.txt} files taken from the Project Gutenberg\parencite{Gutenberg} online library.
	The Project Workflow is the following:
	\begin{enumerate}
		\item Parsing all the files using Hadoop Map Reduce job: 
		\begin{itemize}
			\item Mapper performs the counting of all the specified language parts in each file. These parts are ArrayList instances. The Mapper associates each language part with an integer flag set to 1. The collected data are explained more in detail in the next sections. 
			
			\item Reducer performs the counting of language parts by putting together data taken from the Mappers. The reducer output  
		\end{itemize}
		\item Calculating the frequencies of language parts for each text.
		\item 
	\end{enumerate}

	

	\subsection{Map Reduce Job}

	\subsection{Frequency Calculations}

	\subsection{Similarity Analysis}

	\newpage

	\printbibheading
	\printbibliography[nottype=book,heading=subbibliography,title={Online Sources}]





\end{document}